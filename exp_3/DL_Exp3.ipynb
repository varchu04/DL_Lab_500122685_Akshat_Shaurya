{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"YRnc5NMOW_2G","executionInfo":{"status":"error","timestamp":1761901687230,"user_tz":-330,"elapsed":23366,"user":{"displayName":"AAYUSH ASHUTOSH","userId":"02125907568501155693"}},"outputId":"97ba5149-5f14-4593-f685-ef17fc111e60"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Semester5/Experiment2/BMW_Car_Sales_Classification.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1230401565.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Step 1: Load and Preprocess Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load dataset from CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Semester5/Experiment2/BMW_Car_Sales_Classification.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Display first few rows and basic info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Semester5/Experiment2/BMW_Car_Sales_Classification.csv'"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","\n","# Step 1: Load and Preprocess Dataset\n","# Load dataset from CSV\n","data = pd.read_csv('/content/drive/MyDrive/Semester5/Experiment2/BMW_Car_Sales_Classification.csv')\n","\n","# Display first few rows and basic info\n","print(\"Dataset Shape:\", data.shape)\n","print(\"\\nFirst 5 rows:\")\n","print(data.head())\n","print(\"\\nDataset Info:\")\n","data.info()\n","\n","# Check for missing values\n","print(\"\\nMissing values:\")\n","print(data.isnull().sum())\n","\n","# Separate features and target\n","X = data.drop('Sales_Classification', axis=1)\n","y = data['Sales_Classification']\n","\n","# Encode target variable\n","le = LabelEncoder()\n","y = le.fit_transform(y)  # 'High' -> 1, 'Low' -> 0\n","\n","# Identify categorical and numerical columns\n","categorical_cols = ['Model', 'Region', 'Color', 'Fuel_Type', 'Transmission']\n","numerical_cols = ['Year', 'Engine_Size_L', 'Mileage_KM', 'Price_USD', 'Sales_Volume']\n","\n","# Create preprocessing pipeline\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numerical_cols),\n","        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n","    ])\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Apply preprocessing\n","X_train = preprocessor.fit_transform(X_train)\n","X_test = preprocessor.transform(X_test)\n","\n","# Convert to dense arrays (if sparse)\n","X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n","X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n","\n","# Get input shape\n","input_shape = (X_train.shape[1],)\n","print(f\"\\nInput shape: {input_shape}\")\n","\n","# Step 2: Build Neural Network Model\n","model = models.Sequential([\n","    # Input layer\n","    layers.Dense(128, activation='relu', input_shape=input_shape),\n","    layers.Dropout(0.3),\n","\n","    # Hidden layers\n","    layers.Dense(64, activation='relu'),\n","    layers.Dropout(0.3),\n","    layers.Dense(32, activation='relu'),\n","    layers.Dropout(0.2),\n","\n","    # Output layer (binary classification)\n","    layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Display model architecture\n","model.summary()\n","\n","# Step 3: Compile the Model\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Step 4: Train the Model\n","history = model.fit(X_train, y_train,\n","                    epochs=50,\n","                    batch_size=32,\n","                    validation_split=0.2,\n","                    verbose=1)\n","\n","# Step 5: Evaluate the Model\n","test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n","print(f'\\nTest accuracy: {test_acc:.4f}')\n","print(f'Test loss: {test_loss:.4f}')\n","\n","# Step 6: Plot Accuracy and Loss Curves\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n","# Step 7: Generate Confusion Matrix\n","# Get predictions\n","y_pred_prob = model.predict(X_test)\n","y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n","\n","# Compute confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Step 8: Calculate Performance Metrics\n","# Calculate metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)  # Sensitivity\n","f1 = f1_score(y_test, y_pred)\n","\n","# Calculate specificity\n","TN, FP, FN, TP = conf_matrix.ravel()\n","specificity = TN / (TN + FP)\n","\n","# Print metrics\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall (Sensitivity): {recall:.4f}\")\n","print(f\"Specificity: {specificity:.4f}\")\n","print(f\"F1-Score: {f1:.4f}\")\n","\n","# Detailed classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=['Low', 'High']))\n","\n","# Step 9: Feature Importance (Optional)\n","# Get feature names after preprocessing\n","feature_names = []\n","for name, transformer, columns in preprocessor.transformers_:\n","    if name == 'num':\n","        feature_names.extend(columns)\n","    elif name == 'cat':\n","        # Get one-hot encoded feature names\n","        cat_features = transformer.get_feature_names_out(categorical_cols)\n","        feature_names.extend(cat_features)\n","\n","# Get weights from first layer\n","weights = model.layers[0].get_weights()[0]\n","importance = np.mean(np.abs(weights), axis=1)\n","\n","# Create DataFrame for visualization\n","feature_importance = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Importance': importance\n","}).sort_values('Importance', ascending=False)\n","\n","# Plot top 20 features\n","plt.figure(figsize=(10, 8))\n","sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n","plt.title('Top 20 Feature Importances')\n","plt.tight_layout()\n","plt.show()"]}]}